### Title
MERITS: Metaresearch Evaluation Repository to Identify Trustworthy Science

### Abstract
In recent years, there has been an explosion of interest in post-publication peer-review, with many models proposing multidimensional article-level ratings (e.g., Kriegeskorte, 2012, Frontiers in Computer Neuroscience) as an alternative to unidimensional journal-level metrics (e.g, the journal impact factor). In line with these ideas, a growing number of journals (e.g. F1000) and preprint review platforms now solicit reviewers’ ratings on various dimensions of interest (e.g. PREreview, Plaudit, Rapid Reviews Covid-19). These ratings remain siloed within each project, however, limiting the interoperability, searchability, and comparison between sites, and preventing research that could otherwise be conducted into the nature of these ratings and how they relate to real-world outcomes (e.g. citations, patents, replicability). Crucially, the low visibility of preprint ratings makes it hard for readers to identify whether preprints have been evaluated, such as the large number of low-quality preprints that have been shared and promoted throughout the COVID pandemic. Project XXXXX aims to solve these problems by providing a central, machine-readable database to store and find preprint ratings on a range of dimensions. This serves three key purposes: (1) to amalgamate ratings from different article and preprint review platforms into a single location; (2) to help users (e.g., researchers, journalists) identify and find peer ratings (e.g., by DOI search); and (3) to foster metaresearch and innovation in the scholarly evaluation space. In this presentation, I'll provide an overview of the prototype database and user interface we recently developed and any progress we have made since the submission of this abstract. I'll then outline our plans for the project and future use cases, for example allowing researchers to enter rating data from metaresearch projects (e.g., RepliCATS) or those commissioned by journals during the peer review process. I’ll also provide an overview of our plans to conduct metaresearch on the data we pull into the database, for example exploring how different ratings relate to one another and whether they can predict real-world outcomes of interest. Ultimately, we hope that this project will spur innovation in the scholarly evaluation space, such that we can move beyond the coarse, unidimensional, journal-based metrics of today (e.g., journal impact factor) and toward a more inclusive and evidence-based system in which articles are evaluated on their own merits according to a range of flexible, dynamic and field-specific metrics.
