#### Title
MERITS: Metaresearch Evaluation Repository to Identify Trustworthy Science

#### Abstract
In recent years, there has been an explosion of interest in post-publication peer-review, with many models proposing multidimensional article-level ratings (e.g., Kriegeskorte, 2012, Frontiers in Computer Neuroscience) as an alternative to unidimensional journal-level metrics (e.g, the journal impact factor). In line with these ideas, a growing number of journals (e.g. F1000) and preprint review platforms now solicit reviewersâ€™ ratings on various dimensions of interest (e.g. PREreview, Plaudit, Rapid Reviews Covid-19). These ratings remain siloed within each project, however, limiting the interoperability, searchability, and comparison between sites, and preventing research that could otherwise be conducted into the nature of these ratings and how they relate to real-world outcomes (e.g. citations, patents, replicability). Crucially, the low visibility of preprint ratings makes it hard for readers to identify whether preprints have been evaluated, such as the large number of low-quality preprints that have been shared and promoted throughout the COVID pandemic (relevant to the 'Science communication through social media' track). MERITS aims to solve these problems by providing a central, machine-readable database to store and find preprint ratings on a range of dimensions. MERITS serves three key purposes: (1) to amalgamate ratings from different preprint review platforms into a single location  (relevant to the 'Research assessment' track); (2) to help researchers/journalists identify and find peer ratings of articles/preprints (relevant to the 'Science Discoverability' track); and (3) to foster metaresearch and innovation in the scholarly evaluation space  (relevant to the 'Research assessment' track and 'The Future of Scholarly Communication' theme). In this presentation, I'll provide an overview of the prototype database and user interface we developed at the recent eLife Innovation Sprint and our progress since. I'll then outline our plans for the project and future use cases, for example allowing researchers to enter data collected during metaresearch projects (e.g., RepliCATS) and our plans to use the database to conduct metaresearch into the nature of metrics (relevant to 'The Future of Scholarly Communication' theme). Ultimately, we hope that MERITS will spur innovation in the scholarly evaluation space, such that we can move beyond the coarse, unidimensional, journal-based metrics of today (e.g., journal impact factor) and toward a more inclusive and evidence-based system in which articles are evaluated on their own merits by the 'crowd' according to a range of flexible, dynamic and field-specific metrics (relevant to the 'Systemic change for equity and inclusion' track). 
